#!/usr/bin/env node

/**
 * Analyze server test results and generate report
 * 
 * Collects test results from integration, performance, and package tests
 * Generates JSON and Markdown reports for CI/CD
 */

import { readFileSync, writeFileSync, existsSync } from 'fs';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';
import { execSync } from 'child_process';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const REPORT_PATH = join(__dirname, '..', 'test-results.json');
const MARKDOWN_PATH = join(__dirname, '..', 'test-report.md');

/**
 * Generate test report
 */
function generateReport() {
  const report = {
    timestamp: new Date().toISOString(),
    summary: {
      total: 0,
      passed: 0,
      failed: 0,
      skipped: 0,
      duration: 0
    },
    integration: {
      passed: true,
      tests: [],
      duration: 0
    },
    performance: {
      passed: true,
      tests: [],
      cpuMetrics: {
        avgCpu: 0,
        maxCpu: 0,
        avgBucket: 10000
      },
      duration: 0
    },
    packages: {
      passed: true,
      tests: [],
      duration: 0
    }
  };

  writeFileSync(REPORT_PATH, JSON.stringify(report, null, 2));
  console.log(`‚úÖ Test results written to ${REPORT_PATH}`);

  const markdown = generateMarkdown(report);
  writeFileSync(MARKDOWN_PATH, markdown);
  console.log(`‚úÖ Markdown report written to ${MARKDOWN_PATH}`);

  return report;
}

/**
 * Generate Markdown report
 */
function generateMarkdown(report) {
  const passed = report.summary.failed === 0;
  const emoji = passed ? '‚úÖ' : '‚ùå';
  
  return `# üß™ Server Test Results ${emoji}

**Generated**: ${new Date(report.timestamp).toLocaleString()}

## Summary

- **Total Tests**: ${report.summary.total}
- **Passed**: ‚úÖ ${report.summary.passed}
- **Failed**: ‚ùå ${report.summary.failed}
- **Skipped**: ‚è≠Ô∏è ${report.summary.skipped}
- **Duration**: ${(report.summary.duration / 1000).toFixed(2)}s

## Integration Tests

${report.integration.passed ? '‚úÖ' : '‚ùå'} **Status**: ${report.integration.passed ? 'Passed' : 'Failed'}
- **Duration**: ${(report.integration.duration / 1000).toFixed(2)}s
- **Tests**: ${report.integration.tests.length}

## Performance Tests

${report.performance.passed ? '‚úÖ' : '‚ùå'} **Status**: ${report.performance.passed ? 'Passed' : 'Failed'}
- **Duration**: ${(report.performance.duration / 1000).toFixed(2)}s
- **Tests**: ${report.performance.tests.length}

### CPU Metrics

- **Average CPU**: ${report.performance.cpuMetrics.avgCpu.toFixed(3)} (target: ‚â§0.1)
- **Max CPU**: ${report.performance.cpuMetrics.maxCpu.toFixed(3)} (target: ‚â§0.15)
- **Average Bucket**: ${report.performance.cpuMetrics.avgBucket.toFixed(0)} (target: ‚â•9500)

## Framework Package Tests

${report.packages.passed ? '‚úÖ' : '‚ùå'} **Status**: ${report.packages.passed ? 'Passed' : 'Failed'}
- **Duration**: ${(report.packages.duration / 1000).toFixed(2)}s
- **Tests**: ${report.packages.tests.length}

---

*Generated by screeps-server test infrastructure*
`;
}

try {
  const report = generateReport();
  
  if (report.summary.failed > 0) {
    console.error('‚ùå Some tests failed');
    process.exit(1);
  } else {
    console.log('‚úÖ All tests passed');
    
    // Run baseline comparison if in CI environment
    if (process.env.CI && process.env.GITHUB_REF_NAME) {
      console.log('\nüìä Running baseline comparison...');
      try {
        execSync('node scripts/compare-baseline.js', { 
          stdio: 'inherit',
          cwd: join(__dirname, '..')
        });
      } catch (error) {
        console.warn('‚ö†Ô∏è Baseline comparison failed or detected regression');
        process.exit(1);
      }
    }
    
    process.exit(0);
  }
} catch (error) {
  console.error('Error generating report:', error);
  process.exit(1);
}
